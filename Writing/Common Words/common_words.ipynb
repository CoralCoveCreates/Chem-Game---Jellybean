{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Common Words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to provide the means to keep the reading level of the game dialogue and various written elements to a fairly accessible reading level. An exception is made for the necessity of providing words specific to chemistry and the sciences. The last section of this notebook shows how the various created dictionaries are used to profile samples of text to see if they are using language that is beyond the scope of the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kay\\anaconda3\\envs\\tidalorbit\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import words\n",
    "from collections import Counter\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.7)\n",
      "Path to dataset files: C:\\Users\\Kay\\.cache\\kagglehub\\datasets\\rtatman\\english-word-frequency\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rtatman/english-word-frequency\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "filename = path + \"\\\\\" + \"unigram_freq.csv\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "#df.head()\n",
    "#df.tail()\n",
    "\n",
    "#\"would\" in words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = words.words()\n",
    "word_dict = { word: \"\" for word in all_words}\n",
    "my_suspect_list = list(df['word'])\n",
    "bools_list = [ True if x in word_dict else False for x in my_suspect_list ]\n",
    "#print(bools_list[0:10])\n",
    "Counter(bools_list)\n",
    "df['is_word'] = bools_list\n",
    "#df.head()\n",
    "\n",
    "df = df.drop(df[df.is_word == False].index)\n",
    "#set(df['is_word'])\n",
    "\n",
    "df.drop('is_word', axis=1,inplace=True)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Limited Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "word_list_limited = list(df[0:12000]['word'])\n",
    "word_list = list(df[0:23000]['word'])\n",
    "limited_dict = { word: \"\" for word in word_list_limited}\n",
    "larger_dict = { word: \"\" for word in word_list}\n",
    "full_dict = word_dict\n",
    "\n",
    "print(len(limited_dict))\n",
    "print('bailey' in limited_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Chemistry Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def make_suspect_list (my_list):\n",
    "    my_list = [word.lower() for word in my_list]\n",
    "    my_list = list(set(my_list))\n",
    "    suspect_words_l = [word if not word in limited_dict else \"\" for word in my_list]\n",
    "    suspect_words_l = list(set(suspect_words_l))\n",
    "    # printing the data \n",
    "    #print(my_list) \n",
    "    #print(suspect_words_l)\n",
    "    return suspect_words_l\n",
    "\n",
    "def make_non_suspect_list_from_larger_dict(my_list):\n",
    "    my_list = [word.lower() for word in my_list]\n",
    "    my_list = list(set(my_list))\n",
    "    non_suspect_words_l = [word if word in larger_dict else \"\" for word in my_list]\n",
    "    non_suspect_words_l = list(set(non_suspect_words_l))\n",
    "    return non_suspect_words_l\n",
    "\n",
    "def make_suspect_list_from_full_dict(my_list):\n",
    "    my_list = [word.lower() for word in my_list]\n",
    "    my_list = list(set(my_list))\n",
    "    suspect_words_l = [word if not word in full_dict else \"\" for word in my_list]\n",
    "    suspect_words_l = list(set(suspect_words_l))\n",
    "    return suspect_words_l\n",
    "\n",
    "def make_non_suspect_list_from_full_dict(my_list):\n",
    "    my_list = [word.lower() for word in my_list]\n",
    "    my_list = list(set(my_list))\n",
    "    non_suspect_words_l = [word if word in full_dict else \"\" for word in my_list]\n",
    "    non_suspect_words_l = list(set(non_suspect_words_l))\n",
    "    return non_suspect_words_l\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def open_and_split_txtfile(myfilename):\n",
    "    # opening the file in read mode \n",
    "    my_file = open(myfilename, \"r\") \n",
    "  \n",
    "    # reading the file \n",
    "    data = my_file.read() \n",
    "  \n",
    "    # replacing end of line('/n') with ' ' and \n",
    "    # splitting the text it further when '.' is seen. \n",
    "    data = data.replace('\\n', ' ')\n",
    "    data = data.replace(',', '')\n",
    "    data = data.replace(\"\\'\", '')\n",
    "    data = data.replace(\"\\\"\", '')\n",
    "    data = data.replace('.', '')\n",
    "    data = data.replace('(', '')\n",
    "    data = data.replace(')', '')\n",
    "    my_list = data.split(\" \") \n",
    "    my_file.close()\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "def open_txtfile(myfilename):\n",
    "        # opening the file in read mode \n",
    "    my_file = open(myfilename, \"r\") \n",
    "  \n",
    "    # reading the file \n",
    "    data = my_file.read() \n",
    "    my_file.close()\n",
    "    my_list = data\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "pdf_file = \"..\\\\Chemistry Books\\\\Chemistry2e-WEB.pdf\"\n",
    "reader = PyPDF2.PdfReader(pdf_file)\n",
    "number_of_pages = len(reader.pages)\n",
    "for page_number in range(number_of_pages):   \n",
    "    page = reader.pages[page_number].extract_text().split(\" \")  # Extract page wise text then split based on spaces as required by you\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = ['- \\n', ' \\n', '\\n', '- ']\n",
    "\n",
    "for c in (chr(i) for i in range(33,32 +33)):\n",
    "    symbols.append(c)\n",
    "\n",
    "symbols.remove('-')\n",
    "\n",
    "pages = []\n",
    "pdf_file = \"..\\\\Chemistry Books\\\\Chemistry2e-WEB.pdf\"\n",
    "reader = PyPDF2.PdfReader(pdf_file)\n",
    "number_of_pages = len(reader.pages)\n",
    "for page_number in range(number_of_pages):   \n",
    "    page = reader.pages[page_number]\n",
    "    page = page.extract_text()\n",
    "    for symbol in symbols: \n",
    "        page = page.replace(symbol, \"\")\n",
    "    page = page.split(\" \")  # Extract page wise text then split based on spaces as required by you\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = []\n",
    "\n",
    "book_list = flatten(pages)\n",
    "book_list = list(set(book_list))\n",
    "suspect_l = make_suspect_list(book_list)\n",
    "chemistry_words_l = make_non_suspect_list_from_full_dict(suspect_l)\n",
    "chemistry_words_l.sort(key=len)\n",
    "chemistry_words_l = [word if len(word) > 3 else \"\" for word in chemistry_words_l]\n",
    "#with open(\"Chemistry_Words.txt\", \"w\") as output:\n",
    "    #output.write(str(chemistry_words_l))\n",
    "\n",
    "#output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "924"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chemistry_manual_l = open_and_split_txtfile(\"Chemistry_Words_Manually_Refined.txt\")\n",
    "len(chemistry_manual_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_dict = { word: \"\" for word in chemistry_manual_l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sixth Grade Words Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6516"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sixthGradeWords_l = open_and_split_txtfile(\"1st through 6th grade vocabulary.txt\")\n",
    "sixthGradeWords_l = list(set(sixthGradeWords_l))\n",
    "len(sixthGradeWords_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "middleschool_words_l = make_non_suspect_list_from_full_dict(sixthGradeWords_l)\n",
    "middleschool_words_l.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6087\n"
     ]
    }
   ],
   "source": [
    "print(len(middleschool_words_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "middleschool_dict = { word: \"\" for word in middleschool_words_l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Text Sample Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textTester(sample_text_list):\n",
    "    \n",
    "    suspect_words_l = []\n",
    "    sample_text_list = [word.lower() for word in sample_text_list]\n",
    "    sample_text_list = list(set(sample_text_list))\n",
    "\n",
    "    for word in sample_text_list:\n",
    "        if (not word in limited_dict):\n",
    "             if (not word in chem_dict):\n",
    "                if (not word in middleschool_dict): \n",
    "                    suspect_words_l.append(word)\n",
    "\n",
    "    suspect_words_l = list(set(suspect_words_l))\n",
    "    return suspect_words_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 211\n",
      "Number of suspect words: 38\n",
      "Here are some of the suspects: ['fields', 'bonds', 'changes', 'areas', 'occupies', 'rocks', 'compounds', 'aspects', 'evolved', 'explains']\n"
     ]
    }
   ],
   "source": [
    "sample_text_l = open_and_split_txtfile(\"sample_text_from_wikipedia\")\n",
    "print(\"Original text length:\",  len(sample_text_l))\n",
    "suspects_l = textTester(sample_text_l)\n",
    "print(\"Number of suspect words:\", len(suspects_l))\n",
    "print(\"Here are some of the suspects:\", suspects_l[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"candidate_words.txt\", \"w\") as output:\n",
    "#    output.write(str(suspects_l))\n",
    "\n",
    "#output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tidalorbit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
